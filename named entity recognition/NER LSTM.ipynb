{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "tf.keras.utils.set_random_seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        data = np.array([line.strip() for line in file.readlines()])\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_data('data/large/train/sentences.txt')\n",
    "train_labels = load_data('data/large/train/labels.txt')\n",
    "\n",
    "val_sentences = load_data('data/large/val/sentences.txt')\n",
    "val_labels = load_data('data/large/val/labels.txt')\n",
    "\n",
    "test_sentences = load_data('data/large/test/sentences.txt')\n",
    "test_labels = load_data('data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'world', 'this', 'message', 'is', 'hello', 'a']\n",
      "tf.Tensor([6 1 2], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vec = tf.keras.layers.TextVectorization(standardize=None)\n",
    "vec.adapt(['hello world this is a message'])\n",
    "print(vec.get_vocabulary())\n",
    "print(vec('hello there world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vectorizer(sentences):\n",
    "    vec = tf.keras.layers.TextVectorization(standardize=None)\n",
    "    vec.adapt(sentences)\n",
    "\n",
    "    return vec, vec.get_vocabulary()\n",
    "\n",
    "sentence_vectorizer, vocab = get_sentence_vectorizer(sentences=train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O',\n",
       "       'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O',\n",
       "       'O O O O O O O O O O O B-geo I-geo O', ...,\n",
       "       'B-per I-per O O O B-tim O O O O O O O O O O',\n",
       "       'B-gpe O B-per I-per O O O O O B-org I-org I-org O O O O',\n",
       "       'O O O O O O B-geo O O O O O O O O O O O O O O O O'], dtype='<U287')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(labels):\n",
    "    all_tags = set()\n",
    "\n",
    "    for label in labels:\n",
    "        tags = label.split(' ')\n",
    "        all_tags = all_tags.union(tags)\n",
    "    \n",
    "    all_tags = sorted(list(all_tags))\n",
    "    return all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tags(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art': 0,\n",
       " 'B-eve': 1,\n",
       " 'B-geo': 2,\n",
       " 'B-gpe': 3,\n",
       " 'B-nat': 4,\n",
       " 'B-org': 5,\n",
       " 'B-per': 6,\n",
       " 'B-tim': 7,\n",
       " 'I-art': 8,\n",
       " 'I-eve': 9,\n",
       " 'I-geo': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-nat': 12,\n",
       " 'I-org': 13,\n",
       " 'I-per': 14,\n",
       " 'I-tim': 15,\n",
       " 'O': 16}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tag_map(tags):\n",
    "    tag_map = {}\n",
    "    for i, tag in enumerate(tags):\n",
    "        tag_map[tag] = i\n",
    "    return tag_map\n",
    "\n",
    "tag_map = make_tag_map(get_tags(train_labels))\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vectorizer(labels, tag_map):\n",
    "    label_ids = []\n",
    "    for item in labels:\n",
    "        label_ids.append(list(map(lambda tag: tag_map[tag], item.split(' '))))\n",
    "\n",
    "    label_ids = tf.keras.utils.pad_sequences(label_ids, padding='post', value=-1)\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'\n",
      " 'Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"']\n",
      "Labels: ['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O'\n",
      " 'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O']\n",
      "[0] last 10 tokens: [16 16 16 16 -1 -1 -1 -1 -1 -1]\n",
      "[1] last 10 tokens: [16 16 16 16 16 16 16 16 16 16]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[0:2]}\")\n",
    "print(f\"Labels: {train_labels[0:2]}\")\n",
    "\n",
    "vec_labels = label_vectorizer(train_labels[0:2], tag_map)\n",
    "print(f'[0] last 10 tokens: {vec_labels[0][-10:]}')\n",
    "print(f'[1] last 10 tokens: {vec_labels[1][-10:]}')\n",
    "# print(len(vec_labels[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(sentences, labels, sentence_vectorizer, tag_map):\n",
    "    sentence_ids = sentence_vectorizer(sentences)\n",
    "    labels_ids = label_vectorizer(labels, tag_map = tag_map)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sentence_ids, labels_ids))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(train_sentences, train_labels, sentence_vectorizer, tag_map)\n",
    "val_dataset = generate_dataset(val_sentences, val_labels,  sentence_vectorizer, tag_map)\n",
    "test_dataset = generate_dataset(test_sentences, test_labels,  sentence_vectorizer, tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(len_tags, vocab_size, embedding_dim=50):\n",
    "    model = tf.keras.Sequential(name = 'sequential') \n",
    "\n",
    "    # add one to vocab_size if mask_zero = True\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size + 1, embedding_dim, mask_zero=True))\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(units=embedding_dim, return_sequences=True))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=len_tags, activation=tf.nn.log_softmax))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built-in loss functions do not provide options to ignore mask values. Our masked loss function allows as to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, ignore_class=-1)\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    \n",
    "    return  loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32) \n",
    "\n",
    "    # Create mask for non-masked values\n",
    "    mask = tf.not_equal(y_true, -1)\n",
    "    mask = tf.cast(mask, tf.float32) \n",
    "\n",
    "    y_pred_class = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred_class = tf.cast(y_pred_class, tf.float32) \n",
    "\n",
    "    matches_true_pred  = tf.equal(y_true, y_pred_class)\n",
    "    matches_true_pred = tf.cast(matches_true_pred , tf.float32) \n",
    "\n",
    "    # apply mask to create predictions that ignore masked values\n",
    "    matches_true_pred *= mask\n",
    "\n",
    "    masked_acc = tf.reduce_sum(matches_true_pred) / tf.maximum(tf.reduce_sum(mask), 1)\n",
    "\n",
    "    return masked_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1492400   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 50)          20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 17)          867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,513,467\n",
      "Trainable params: 1,513,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NER(len(tag_map), len(vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.01), \n",
    "    loss = masked_loss,\n",
    "    metrics = [masked_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525/525 [==============================] - 18s 16ms/step - loss: 0.0576 - masked_accuracy: 0.9288 - val_loss: 0.0422 - val_masked_accuracy: 0.9579\n",
      "Epoch 2/10\n",
      "525/525 [==============================] - 7s 13ms/step - loss: 0.0233 - masked_accuracy: 0.9657 - val_loss: 0.0405 - val_masked_accuracy: 0.9587\n",
      "Epoch 3/10\n",
      "525/525 [==============================] - 8s 14ms/step - loss: 0.0189 - masked_accuracy: 0.9708 - val_loss: 0.0419 - val_masked_accuracy: 0.9580\n",
      "Epoch 4/10\n",
      "525/525 [==============================] - 7s 13ms/step - loss: 0.0166 - masked_accuracy: 0.9739 - val_loss: 0.0431 - val_masked_accuracy: 0.9583\n",
      "Epoch 5/10\n",
      "525/525 [==============================] - 7s 14ms/step - loss: 0.0150 - masked_accuracy: 0.9761 - val_loss: 0.0450 - val_masked_accuracy: 0.9582\n",
      "Epoch 6/10\n",
      "525/525 [==============================] - 7s 13ms/step - loss: 0.0137 - masked_accuracy: 0.9779 - val_loss: 0.0471 - val_masked_accuracy: 0.9567\n",
      "Epoch 7/10\n",
      "525/525 [==============================] - 7s 13ms/step - loss: 0.0127 - masked_accuracy: 0.9794 - val_loss: 0.0499 - val_masked_accuracy: 0.9563\n",
      "Epoch 8/10\n",
      "525/525 [==============================] - 7s 13ms/step - loss: 0.0118 - masked_accuracy: 0.9806 - val_loss: 0.0524 - val_masked_accuracy: 0.9547\n",
      "Epoch 9/10\n",
      "525/525 [==============================] - 7s 12ms/step - loss: 0.0111 - masked_accuracy: 0.9816 - val_loss: 0.0548 - val_masked_accuracy: 0.9545\n",
      "Epoch 10/10\n",
      "525/525 [==============================] - 7s 13ms/step - loss: 0.0105 - masked_accuracy: 0.9827 - val_loss: 0.0559 - val_masked_accuracy: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2106a3f0d90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "model.fit(\n",
    "    train_dataset.batch(BATCH_SIZE),\n",
    "    validation_data=val_dataset.batch(BATCH_SIZE),\n",
    "    shuffle=True,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1633 2112   22 ...    0    0    0]\n",
      " [2086   65    6 ...    0    0    0]\n",
      " [  48 1733 5215 ...    0    0    0]\n",
      " ...\n",
      " [ 400   44   19 ...    0    0    0]\n",
      " [ 684   50 2819 ...    0    0    0]\n",
      " [ 203   25   41 ...    0    0    0]], shape=(7194, 70), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test_ids = sentence_vectorizer(test_sentences)\n",
    "print(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 16 16 ... -1 -1 -1]\n",
      " [16 16 16 ... -1 -1 -1]\n",
      " [16 16 16 ... -1 -1 -1]\n",
      " ...\n",
      " [ 3 16 16 ... -1 -1 -1]\n",
      " [16 16 16 ... -1 -1 -1]\n",
      " [16 16 16 ... -1 -1 -1]] (7194, 70)\n"
     ]
    }
   ],
   "source": [
    "test_label_ids = label_vectorizer(test_labels, tag_map)\n",
    "print(test_label_ids, test_label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 2s 7ms/step\n",
      "[[[-8.32166862e+00 -8.63365459e+00 -7.35555053e-01 ... -1.18176727e+01\n",
      "   -9.10229683e+00 -5.75114536e+00]\n",
      "  [-1.53517513e+01 -1.49933100e+01 -1.52658243e+01 ... -1.01397734e+01\n",
      "   -1.01792831e+01 -5.33200160e-04]\n",
      "  [-2.02329636e+01 -1.97180080e+01 -1.34848385e+01 ... -2.18994331e+01\n",
      "   -9.43583107e+00 -1.37699174e-03]\n",
      "  ...\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]]\n",
      "\n",
      " [[-1.49555387e+01 -1.80311718e+01 -1.82787533e+01 ... -1.14925070e+01\n",
      "   -8.59307575e+00 -1.45210640e-03]\n",
      "  [-1.45132151e+01 -1.54445114e+01 -1.43930969e+01 ... -2.19556389e+01\n",
      "   -1.05537558e+01 -1.53538175e-02]\n",
      "  [-1.42145729e+01 -1.10201998e+01 -1.05489779e+01 ... -1.84101124e+01\n",
      "   -7.71041775e+00 -1.46781909e-03]\n",
      "  ...\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]]\n",
      "\n",
      " [[-1.23967943e+01 -1.40851755e+01 -1.12885313e+01 ... -1.72333260e+01\n",
      "   -1.39568052e+01 -2.97984132e-03]\n",
      "  [-1.97311192e+01 -2.01581707e+01 -1.69638157e+01 ... -1.87975788e+01\n",
      "   -1.97587223e+01 -1.38281821e-05]\n",
      "  [-1.93868656e+01 -2.08088474e+01 -1.60378723e+01 ... -1.62730999e+01\n",
      "   -2.20113544e+01 -2.74180979e-06]\n",
      "  ...\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-9.01020527e+00 -1.02113209e+01 -4.97935724e+00 ... -1.76829853e+01\n",
      "   -1.16526728e+01 -6.75370026e+00]\n",
      "  [-1.81712208e+01 -1.68391666e+01 -1.66654778e+01 ... -2.07053013e+01\n",
      "   -1.64762592e+01 -1.31129354e-05]\n",
      "  [-2.92955933e+01 -3.24511642e+01 -2.51133785e+01 ... -3.23156509e+01\n",
      "   -1.84007130e+01  0.00000000e+00]\n",
      "  ...\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]]\n",
      "\n",
      " [[-1.05420160e+01 -9.72934055e+00 -1.05702791e+01 ... -1.29440393e+01\n",
      "   -5.51693010e+00 -7.79138580e-02]\n",
      "  [-2.30509186e+01 -2.00513592e+01 -1.81200047e+01 ... -1.23041620e+01\n",
      "   -5.57824326e+00 -5.58376126e-03]\n",
      "  [-1.80032768e+01 -1.74348869e+01 -1.12479277e+01 ... -1.81860142e+01\n",
      "   -1.15904608e+01 -2.38415741e-05]\n",
      "  ...\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]]\n",
      "\n",
      " [[-1.89566555e+01 -2.22736721e+01 -1.77735538e+01 ... -1.44037933e+01\n",
      "   -1.57476959e+01 -1.01327387e-05]\n",
      "  [-1.99750957e+01 -2.30477600e+01 -1.54497080e+01 ... -1.78699722e+01\n",
      "   -1.64995708e+01 -6.19886396e-06]\n",
      "  [-2.58842278e+01 -2.27269230e+01 -1.80429058e+01 ... -2.16437225e+01\n",
      "   -1.96837692e+01 -3.57627812e-07]\n",
      "  ...\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]\n",
      "  [-2.88843489e+00 -2.92739391e+00 -2.75706005e+00 ... -3.02077365e+00\n",
      "   -3.13804150e+00 -2.34480286e+00]]] (7194, 70, 17)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_ids)\n",
    "print(y_pred, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.95451057, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "accu = masked_accuracy(test_label_ids, y_pred)\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sentence_vectorized = sentence_vectorizer(sentence)\n",
    "    sentence_vectorized = tf.expand_dims(sentence_vectorized, axis=0)\n",
    "\n",
    "    output = model.predict(sentence_vectorized)\n",
    "    outputs = np.argmax(output, axis=-1)\n",
    "    outputs = outputs[0] \n",
    "\n",
    "    labels = list(tag_map.keys()) \n",
    "\n",
    "    pred = [] \n",
    "    for tag_idx in outputs:\n",
    "        pred_label = labels[tag_idx]\n",
    "        pred.append(pred_label)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pred(sentence):\n",
    "    pred = predict(sentence)\n",
    "\n",
    "    print(pd.DataFrame({ 'Token': sentence.split(' '), 'Tag': pred }, index=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "     Token    Tag\n",
      "0       F1      O\n",
      "1    Grand  B-geo\n",
      "2     Prix  I-org\n",
      "3  happens      O\n",
      "4     this      O\n",
      "5  weekend  B-tim\n",
      "6       in      O\n",
      "7  Bahrain  B-geo\n"
     ]
    }
   ],
   "source": [
    "display_pred('F1 Grand Prix happens this weekend in Bahrain')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
