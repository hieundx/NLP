{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "iHX1oSoJ17g3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random as  rnd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "gpu = gpus[0]\n",
        "tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIkQBNoWzUhB"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91x7dY_-z0VJ"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToCbFjlnvjml",
        "outputId": "513f7807-2504-45dd-d319-d427d274d69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"A LOVER'S COMPLAINT\", 'FROM off a hill whose concave womb reworded', 'A plaintful story from a sistering vale,', 'My spirits to attend this double voice accorded,', 'And down I laid to list the sad-tuned tale;']\n"
          ]
        }
      ],
      "source": [
        "lines = []\n",
        "with open('./data/shakespeare_data.txt') as file:\n",
        "  for line in file:\n",
        "    line = line.strip()\n",
        "    if len(line) > 0:\n",
        "      lines.append(line)\n",
        "\n",
        "print(lines[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE-3vvGJz13A"
      },
      "source": [
        "## Create vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoK2SFEkzi9t",
        "outputId": "b1f83939-7046-4f2d-ce2a-52ed5edc09d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "82 words\n",
            "['[UNK]', '', '\\t', '\\n', ' ', '!', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n"
          ]
        }
      ],
      "source": [
        "text = '\\n'.join(lines)\n",
        "vocab = sorted(set(text))\n",
        "vocab.insert(0, \"[UNK]\") # Unknown token for out-of-vocab words\n",
        "vocab.insert(1, \"\") # empty char for padding\n",
        "\n",
        "print(f'{len(vocab)} words')\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNzwk8K10s33"
      },
      "source": [
        "## Encode sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "r9Fu9bwY0F3s"
      },
      "outputs": [],
      "source": [
        "# Very simple implementation\n",
        "encode_ = lambda string: [ vocab.index(char) for char in string ]\n",
        "decode_ = lambda nums: ' '.join([ vocab[index] for index in nums ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYf1cEzU2xM2"
      },
      "source": [
        "Two issues with this implementation:\n",
        "- It does not take into account UTF-8 characters\n",
        "- It throws an error when encoutering unknown word instead of returning index for UNK token\n",
        "\n",
        "To properly handle these, use:\n",
        "- `tf.strings.unicode_split`: this will encode UTF-8 before splitting\n",
        "- `tf.keras.layers.StringLookup`: this takes care of UNK token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "yLzH9YqD2aMC"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line, vocab):\n",
        "  tokens = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
        "  ids = tf.keras.layers.StringLookup(vocabulary=vocab)(tokens)\n",
        "\n",
        "  return ids\n",
        "\n",
        "def text_from_ids(ids, vocab):\n",
        "  tokens = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True)(ids)\n",
        "\n",
        "  return tf.strings.reduce_join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFgJhSGQ4iuz",
        "outputId": "4a3740ca-62e3-46cd-bd97-15fd7fb7d507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IDs: [62 59 66 66 69  4 77 69 72 66 58]\n",
            "Text: b'hello world'\n"
          ]
        }
      ],
      "source": [
        "ids = line_to_tensor('hello world', vocab)\n",
        "print(f'IDs: {ids}')\n",
        "\n",
        "text = text_from_ids(ids, vocab)\n",
        "print(f'Text: {text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWQN3rzc45Hl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dAf1h_s_ZTn"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "aU9JJfr34rWA"
      },
      "outputs": [],
      "source": [
        "train_lines = lines[: -1000]\n",
        "eval_lines = lines[-1000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_OyqQiBGCxa"
      },
      "source": [
        "### Dataset creation procedure\n",
        "1. Convert text to IDs\n",
        "2. Group IDs into batches of SEQUENCE_LENGTH\n",
        "3. Map each sequence of IDs to text input and target\n",
        "4. Batch again into batches of BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt_bu2cn_jwr",
        "outputId": "986ecf1f-4d97-4a0d-d5ac-34d754ed76fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"
          ]
        }
      ],
      "source": [
        "# Convert text to IDs\n",
        "all_ids = line_to_tensor('\\n'.join(['hello world', 'generative AI']), vocab)\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "print(ids_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "isnEluIUA1fp"
      },
      "outputs": [],
      "source": [
        "# Group IDs into batches of sequence_length\n",
        "seq_length = 5\n",
        "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlzJNtnGBI3m",
        "outputId": "99a35793-5ee2-4cbf-e0ae-a2720a0801fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([62 59 66 66 69  4], shape=(6,), dtype=int64)\n",
            "tf.Tensor([77 69 72 66 58  3], shape=(6,), dtype=int64)\n",
            "tf.Tensor([61 59 68 59 72 55], shape=(6,), dtype=int64)\n",
            "tf.Tensor([74 63 76 59  4 27], shape=(6,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for seq in data_generator.take(5):\n",
        "  print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8IKLJ3-C0F5",
        "outputId": "f584c035-98b4-4b11-bfa2-12b8cad269cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['t', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list('tensorflow'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E046Kl1fFmMB",
        "outputId": "39feed82-29b2-4248-be15-7e58d23b6e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[62, 59, 66, 66, 69],\n",
            "       [77, 69, 72, 66, 58]], dtype=int64)>, <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[59, 66, 66, 69,  4],\n",
            "       [69, 72, 66, 58,  3]], dtype=int64)>)\n",
            "(<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[61, 59, 68, 59, 72],\n",
            "       [74, 63, 76, 59,  4]], dtype=int64)>, <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[59, 68, 59, 72, 55],\n",
            "       [63, 76, 59,  4, 27]], dtype=int64)>)\n"
          ]
        }
      ],
      "source": [
        "# Map each sequence to input and target\n",
        "data_xy = data_generator.map(lambda z: split_input_target(z))\n",
        "\n",
        "# Batch again\n",
        "batches = data_xy.batch(2)\n",
        "for batch in batches.take(2):\n",
        "  print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "GI7nMYPZEZ7-"
      },
      "outputs": [],
      "source": [
        "def create_batch_dataset(lines, vocab, seq_length=20, batch_size=64):\n",
        "  single_line_data = '\\n'.join(lines)\n",
        "\n",
        "  all_ids = line_to_tensor(single_line_data, vocab)\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "  # Split IDs into lines of seq_length + 1\n",
        "  data_generator = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "  # Map lines of IDs into batches of (x, y) tuples\n",
        "  dataset_xy = data_generator.map(lambda z: split_input_target(z))\n",
        "\n",
        "  # Split lines of (x, y) tuples into batches of batch_size\n",
        "  dataset = (\n",
        "      dataset_xy\n",
        "        .shuffle(10000)\n",
        "        .batch(batch_size, drop_remainder=True)\n",
        "        # .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  )\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "NW51js9iEtcZ"
      },
      "outputs": [],
      "source": [
        "dataset = create_batch_dataset(train_lines, vocab, seq_length=100, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iaugqv2hHsh0",
        "outputId": "41b54fff-93a5-4d52-a3af-1d63dcb796d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 64\n",
            "tf.Tensor(b\"rime\\nRot and consume themselves in little time.\\n'Were I hard-favour'd, foul, or wrinkled-old,\\nIll-nu\", shape=(), dtype=string)\n",
            "tf.Tensor(b\"ime\\nRot and consume themselves in little time.\\n'Were I hard-favour'd, foul, or wrinkled-old,\\nIll-nur\", shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# there are a total of batch_size pairs in each dataset batch\n",
        "# THIS IS NOT AN ACTUAL LOOP\n",
        "for input, target in dataset.take(1):\n",
        "  print(f'Batch size: {len(input)}')\n",
        "\n",
        "  print(text_from_ids(input[0], vocab))\n",
        "  print(text_from_ids(target[0], vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRULM(tf.keras.Model):\n",
        "    def __init__(self, vocab_size=256, embedding_dim=256, rnn_units=128):\n",
        "        super().__init__(self)\n",
        "\n",
        "        self.embeddding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(units=vocab_size, activation=tf.nn.log_softmax)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=True):\n",
        "        # Use training flag to forward prop when predicting characters\n",
        "        # since we will use trained weights of the model\n",
        "        \n",
        "        x = inputs\n",
        "        x = self.embeddding(x, training=training)\n",
        "\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "        x = self.dense(x, training=training)\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = 82\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN layers\n",
        "rnn_units = 512\n",
        "\n",
        "model = GRULM(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units = rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test prediction on untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: (100,)\n",
            "y: (1, 100, 82)\n"
          ]
        }
      ],
      "source": [
        "for input, target in dataset.take(1):\n",
        "    x = input[0].numpy()\n",
        "\n",
        "    yhat = model(tf.constant([x]), training=False)\n",
        "\n",
        "    print(f'x: {x.shape}')\n",
        "    print(f'y: {yhat.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "20 is the sequence length, 82 is the vocab size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[69 69 57  9  7 68 47 68 33  7 47 34 69 54 72 54 69 37 56 67 69 54 72 54\n",
            " 48 57 57 28 25 48 34 69 68 34 69 57 54 47 70 68 45 47 47 26 47 13 13 29\n",
            " 48  7 37 54 42 48 34  7 54 69 34  9  5 27 49 34  7 69 34 69 69 69 77 76\n",
            " 76 68 23  5 47 23  2  1 37 74 76  7 65 29 68  2 68 55 23 47 34 69 57 57\n",
            " 27 27 48 69], shape=(100,), dtype=int64)\n",
            "tf.Tensor(b'ooc(&nUnG&UHo]r]oKbmo]r]VccB;VHonHoc]UpnSUU?U..CV&K]PVH&]oH(!AWH&oHooowvvn9!U9\\tKtv&kCn\\tna9UHoccAAVo', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sampled_indices = tf.math.argmax(yhat[0], axis=1)\n",
        "print(sampled_indices)\n",
        "print(text_from_ids(sampled_indices, vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compile_model(model):\n",
        "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.00125)\n",
        "    model.compile(optimizer=opt, loss=loss)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "790/790 [==============================] - 16s 17ms/step - loss: 2.0152\n",
            "Epoch 2/20\n",
            "790/790 [==============================] - 15s 19ms/step - loss: 1.4802\n",
            "Epoch 3/20\n",
            "790/790 [==============================] - 15s 17ms/step - loss: 1.3797\n",
            "Epoch 4/20\n",
            "790/790 [==============================] - 15s 18ms/step - loss: 1.3331\n",
            "Epoch 5/20\n",
            "790/790 [==============================] - 15s 18ms/step - loss: 1.3034\n",
            "Epoch 6/20\n",
            "790/790 [==============================] - 15s 18ms/step - loss: 1.2818\n",
            "Epoch 7/20\n",
            "790/790 [==============================] - 15s 18ms/step - loss: 1.2651\n",
            "Epoch 8/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.2522\n",
            "Epoch 9/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.2411\n",
            "Epoch 10/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.2322\n",
            "Epoch 11/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.2248\n",
            "Epoch 12/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.2172\n",
            "Epoch 13/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.2118\n",
            "Epoch 14/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.2065\n",
            "Epoch 15/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.2020\n",
            "Epoch 16/20\n",
            "790/790 [==============================] - 15s 18ms/step - loss: 1.1977\n",
            "Epoch 17/20\n",
            "790/790 [==============================] - 15s 18ms/step - loss: 1.1944\n",
            "Epoch 18/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.1914\n",
            "Epoch 19/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.1887\n",
            "Epoch 20/20\n",
            "790/790 [==============================] - 14s 17ms/step - loss: 1.1859\n"
          ]
        }
      ],
      "source": [
        "model = compile_model(model)\n",
        "history = model.fit(dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: (100,)\n",
            "y: (1, 100, 82)\n"
          ]
        }
      ],
      "source": [
        "for input, target in dataset.take(1):\n",
        "    x = input[0].numpy()\n",
        "\n",
        "    yhat = model(tf.constant([x]))\n",
        "\n",
        "    print(f'x: {x.shape}')\n",
        "    print(f'y: {yhat.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.209201705553149\n"
          ]
        }
      ],
      "source": [
        "# GRADED FUNCTION: log_perplexity\n",
        "def log_perplexity(preds, target):\n",
        "    \"\"\"\n",
        "        preds (tf.Tensor): (1, number of predicted chars, vocab size)\n",
        "        target (tf.Tensor): (1, number of predicted chars)\n",
        "    \"\"\"\n",
        "    PADDING_ID = 0\n",
        "    \n",
        "    vocab_size = preds.shape[-1]\n",
        "\n",
        "    # reshape target to match preds shape\n",
        "    target_1h = tf.one_hot(target, vocab_size)\n",
        "\n",
        "    # this produces the log probabilities of P(w_i|w1, ... w_i-1)\n",
        "    log_p = np.sum(preds * target_1h, axis= -1)\n",
        "\n",
        "    # if target has the form [ 1, 2, 0 ]\n",
        "    # non_pad will be [ 1, 1, 0 ]\n",
        "    non_pad = 1.0 - np.equal(target, PADDING_ID)\n",
        "\n",
        "    # remove log probabilities of padded tokens\n",
        "    # this will turn all log probs of padded tokens to 0\n",
        "    # for example if log_p = [ -1, -2, -3 ] and non_pad = [ 1, 1, 0 ]\n",
        "    # then log_p = [ -1, -2, 0 ]\n",
        "    log_p = log_p * non_pad \n",
        "\n",
        "    # finally take the mean \n",
        "    log_ppx = np.mean(log_p)\n",
        "        \n",
        "    return -log_ppx\n",
        "\n",
        "eval_text = \"\\n\".join(eval_lines)\n",
        "eval_ids = line_to_tensor([eval_text], vocab)\n",
        "input_ids, target_ids = split_input_target(tf.squeeze(eval_ids, axis=0))\n",
        "\n",
        "preds, status = model(tf.expand_dims(input_ids, 0), training=False, states=None, return_state=True)\n",
        "\n",
        "log_ppx = log_perplexity(preds, tf.expand_dims(target_ids, 0))\n",
        "print(log_ppx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GRU will generate the same sentence so we need to use random sampling to make it less repetitive. The technique is called temperature random sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "def temperature_random_sampling(log_probs, temperature=1.0):\n",
        "    u = tf.random.uniform(minval=1e-6, maxval=1.0 - 1e-6, shape=log_probs.shape)\n",
        "    g = -tf.math.log(-tf.math.log(u))\n",
        "    return tf.math.argmax(log_probs + g * temperature, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GenerativeModel(tf.keras.Model):\n",
        "    def __init__(self, model, vocab, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def generate_one_step(self, inputs, states= None):\n",
        "        input_ids = line_to_tensor(inputs, self.vocab)\n",
        "\n",
        "        yhat, states = self.model(input_ids, states, return_state=True)\n",
        "        yhat = yhat[0, -1, :] \n",
        "\n",
        "        predicted_ids = temperature_random_sampling(yhat, self.temperature)\n",
        "        next_char = text_from_ids(predicted_ids, vocab)\n",
        "        # predicted_ids = tf.math.argmax(yhat[0], axis=1)\n",
        "        \n",
        "        return tf.expand_dims(next_char, 0), states\n",
        "\n",
        "    def generate_n_chars(self, num_chars, prefix):\n",
        "        states = None\n",
        "        next_char = tf.constant([prefix])\n",
        "        result = [next_char]\n",
        "        for _ in range(num_chars):\n",
        "            next_char, states = self.generate_one_step(next_char, states=states)\n",
        "            result.append(next_char)\n",
        "\n",
        "        return tf.strings.join(result)[0].numpy().decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Though they are lawful than I hear\n",
            "Are many other m\n",
            "TIN]\n",
            "PRINCE HENRY\tWell, here comes the bird.\n",
            "PISTOL\n",
            "The standing time of seven devil did.\n",
            "HORTENSIO\t[As\n",
            "The walls!\n",
            "An if thine ragges grow asher to try so\n",
            "\n",
            "TIV\tThou art Alacum, father. Farewell; come to the \n",
            "The place,\n",
            "And awe him about contriding water. You \n",
            "The gib my faults.\n",
            "VALENTINE\tI see unto thy greater\n",
            "TNEmb, and Art thou gone to pass her roasted answer\n",
            "To sayion that I say about thy working;\n",
            "And if you'\n",
            "There are all beard; and here she may,\n",
            "Yet I will s\n"
          ]
        }
      ],
      "source": [
        "gen = GenerativeModel(model, vocab)\n",
        "\n",
        "for i in range(10):\n",
        "    print(gen.generate_n_chars(50, \"T\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
