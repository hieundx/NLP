{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iHX1oSoJ17g3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random as  rnd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIkQBNoWzUhB"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91x7dY_-z0VJ"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToCbFjlnvjml",
        "outputId": "513f7807-2504-45dd-d319-d427d274d69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"A LOVER'S COMPLAINT\", 'FROM off a hill whose concave womb reworded', 'A plaintful story from a sistering vale,', 'My spirits to attend this double voice accorded,', 'And down I laid to list the sad-tuned tale;']\n"
          ]
        }
      ],
      "source": [
        "lines = []\n",
        "with open('./data/shakespeare_data.txt') as file:\n",
        "  for line in file:\n",
        "    line = line.strip()\n",
        "    if len(line) > 0:\n",
        "      lines.append(line)\n",
        "\n",
        "print(lines[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE-3vvGJz13A"
      },
      "source": [
        "## Create vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoK2SFEkzi9t",
        "outputId": "b1f83939-7046-4f2d-ce2a-52ed5edc09d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "82 words\n",
            "['[UNK]', '', '\\t', '\\n', ' ', '!', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n"
          ]
        }
      ],
      "source": [
        "text = '\\n'.join(lines)\n",
        "vocab = sorted(set(text))\n",
        "vocab.insert(0, \"[UNK]\") # Unknown token for out-of-vocab words\n",
        "vocab.insert(1, \"\") # empty char for padding\n",
        "\n",
        "print(f'{len(vocab)} words')\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNzwk8K10s33"
      },
      "source": [
        "## Encode sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r9Fu9bwY0F3s"
      },
      "outputs": [],
      "source": [
        "# Very simple implementation\n",
        "encode_ = lambda string: [ vocab.index(char) for char in string ]\n",
        "decode_ = lambda nums: ' '.join([ vocab[index] for index in nums ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYf1cEzU2xM2"
      },
      "source": [
        "Two issues with this implementation:\n",
        "- It does not take into account UTF-8 characters\n",
        "- It throws an error when encoutering unknown word instead of returning index for UNK token\n",
        "\n",
        "To properly handle these, use:\n",
        "- `tf.strings.unicode_split`: this will encode UTF-8 before splitting\n",
        "- `tf.keras.layers.StringLookup`: this takes care of UNK token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yLzH9YqD2aMC"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line, vocab):\n",
        "  tokens = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
        "  ids = tf.keras.layers.StringLookup(vocabulary=vocab)(tokens)\n",
        "\n",
        "  return ids\n",
        "\n",
        "def text_from_ids(ids, vocab):\n",
        "  tokens = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True)(ids)\n",
        "\n",
        "  return tf.strings.reduce_join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFgJhSGQ4iuz",
        "outputId": "4a3740ca-62e3-46cd-bd97-15fd7fb7d507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IDs: [62 59 66 66 69  4 77 69 72 66 58]\n",
            "Text: b'hello world'\n"
          ]
        }
      ],
      "source": [
        "ids = line_to_tensor('hello world', vocab)\n",
        "print(f'IDs: {ids}')\n",
        "\n",
        "text = text_from_ids(ids, vocab)\n",
        "print(f'Text: {text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWQN3rzc45Hl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dAf1h_s_ZTn"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aU9JJfr34rWA"
      },
      "outputs": [],
      "source": [
        "train_lines = lines[: -1000]\n",
        "eval_lines = lines[-1000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_OyqQiBGCxa"
      },
      "source": [
        "### Dataset creation procedure\n",
        "1. Convert text to IDs\n",
        "2. Group IDs into batches of SEQUENCE_LENGTH\n",
        "3. Map each sequence of IDs to text input and target\n",
        "4. Batch again into batches of BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt_bu2cn_jwr",
        "outputId": "986ecf1f-4d97-4a0d-d5ac-34d754ed76fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"
          ]
        }
      ],
      "source": [
        "# Convert text to IDs\n",
        "all_ids = line_to_tensor('\\n'.join(['hello world', 'generative AI']), vocab)\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "print(ids_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "isnEluIUA1fp"
      },
      "outputs": [],
      "source": [
        "# Group IDs into batches of sequence_length\n",
        "seq_length = 5\n",
        "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlzJNtnGBI3m",
        "outputId": "99a35793-5ee2-4cbf-e0ae-a2720a0801fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([62 59 66 66 69  4], shape=(6,), dtype=int64)\n",
            "tf.Tensor([77 69 72 66 58  3], shape=(6,), dtype=int64)\n",
            "tf.Tensor([61 59 68 59 72 55], shape=(6,), dtype=int64)\n",
            "tf.Tensor([74 63 76 59  4 27], shape=(6,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for seq in data_generator.take(5):\n",
        "  print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8IKLJ3-C0F5",
        "outputId": "f584c035-98b4-4b11-bfa2-12b8cad269cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['t', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list('tensorflow'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E046Kl1fFmMB",
        "outputId": "39feed82-29b2-4248-be15-7e58d23b6e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[62, 59, 66, 66, 69],\n",
            "       [77, 69, 72, 66, 58]], dtype=int64)>, <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[59, 66, 66, 69,  4],\n",
            "       [69, 72, 66, 58,  3]], dtype=int64)>)\n",
            "(<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[61, 59, 68, 59, 72],\n",
            "       [74, 63, 76, 59,  4]], dtype=int64)>, <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
            "array([[59, 68, 59, 72, 55],\n",
            "       [63, 76, 59,  4, 27]], dtype=int64)>)\n"
          ]
        }
      ],
      "source": [
        "# Map each sequence to input and target\n",
        "data_xy = data_generator.map(lambda z: split_input_target(z))\n",
        "\n",
        "# Batch again\n",
        "batches = data_xy.batch(2)\n",
        "for batch in batches.take(2):\n",
        "  print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GI7nMYPZEZ7-"
      },
      "outputs": [],
      "source": [
        "def create_batch_dataset(lines, vocab, seq_length=20, batch_size=64):\n",
        "  single_line_data = '\\n'.join(lines)\n",
        "\n",
        "  all_ids = line_to_tensor(single_line_data, vocab)\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "  # Split IDs into lines of seq_length + 1\n",
        "  data_generator = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "  # Map lines of IDs into batches of (x, y) tuples\n",
        "  dataset_xy = data_generator.map(lambda z: split_input_target(z))\n",
        "\n",
        "  # Split lines of (x, y) tuples into batches of batch_size\n",
        "  dataset = (\n",
        "      dataset_xy\n",
        "        .shuffle(10000)\n",
        "        .batch(batch_size, drop_remainder=True)\n",
        "        # .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  )\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NW51js9iEtcZ"
      },
      "outputs": [],
      "source": [
        "dataset = create_batch_dataset(train_lines, vocab, seq_length=20, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iaugqv2hHsh0",
        "outputId": "41b54fff-93a5-4d52-a3af-1d63dcb796d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 64\n",
            "tf.Tensor(b'e, we burn daylight,', shape=(), dtype=string)\n",
            "tf.Tensor(b', we burn daylight, ', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# there are a total of batch_size pairs in each dataset batch\n",
        "# THIS IS NOT AN ACTUAL LOOP\n",
        "for input, target in dataset.take(1):\n",
        "  print(f'Batch size: {len(input)}')\n",
        "\n",
        "  print(text_from_ids(input[0], vocab))\n",
        "  print(text_from_ids(target[0], vocab))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
